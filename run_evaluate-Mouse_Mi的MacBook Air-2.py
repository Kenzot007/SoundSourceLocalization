import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import numpy as np
import glob
import random
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, random_split
import re
from tqdm.notebook import tqdm
import ipywidgets as widgets
widgets.IntProgress(value=50, min=0, max=100)
import torch.nn.functional as F
import matplotlib.pyplot as plt

from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

class BinauralCueDataset(Dataset):
    def __init__(self, npz_dir, audio_ids=range(1, 701)):
        self.dir = npz_dir
        pattern = re.compile(r'main_audio_(\d+)_azi(\d+)\.npz')
        self.files = []
        for f in os.listdir(npz_dir):
            if f.endswith('.npz'):
                match = pattern.match(f)
                if match and int(match.group(1)) in audio_ids:
                    self.files.append(f)
        self.files.sort()

        print(f"ğŸ“ å·²åŠ è½½ {len(self.files)} ä¸ª .npz æ–‡ä»¶ï¼Œå…± {len(self)} ä¸ªæ ·æœ¬ã€‚")

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        path = os.path.join(self.dir, self.files[idx])
        data = np.load(path)
        itd = data["itd"].astype(np.float32)
        ild = data["ild"].astype(np.float32)
        ic = data["ic"].astype(np.float32)

        cue = np.stack([itd, ild, ic], axis=0)  # [3, filters, frames]

        # æå– azimuth label
        azimuth = int(re.search(r'azi(\d+)', self.files[idx]).group(1))
        label = azimuth // 5  # å…±72ç±»ï¼ˆ0-71ï¼‰

        return cue, label
    

# ä¸€ç»´å·ç§¯åˆ†æ”¯ï¼šåŒ…å« Conv1d å±‚ã€BatchNorm1d å’Œ ReLU æ¿€æ´»
class ConvBranch(nn.Module):
    def __init__(self, input_channels=32, conv_channels=64, kernel_size=5, stride=1, num_layers=2, use_batchnorm=True):
        super(ConvBranch, self).__init__()
        layers = []
        in_ch = input_channels
        for i in range(num_layers):
            out_ch = conv_channels
            layers.append(nn.Conv1d(in_ch, out_ch, kernel_size, stride, padding=kernel_size//2))
            if use_batchnorm:
                layers.append(nn.BatchNorm1d(out_ch))
            layers.append(nn.ReLU(inplace=True))
            in_ch = out_ch
        self.conv = nn.Sequential(*layers)
    def forward(self, x):
        # x: [B, input_channels, L]  ï¼ˆLä¸ºæ—¶é—´é•¿åº¦ï¼Œå¦‚44100ï¼‰
        return self.conv(x)         # è¾“å‡º: [B, conv_channels, L]

# è‡ªæ³¨æ„åŠ›æ± åŒ–ï¼šå°†å¯å˜é•¿åº¦çš„æ—¶é—´åºåˆ—ç‰¹å¾åŠ æƒæ±‡èšä¸ºä¸€ä¸ªå›ºå®šå‘é‡
class SelfAttentionPooling(nn.Module):
    def __init__(self, embed_dim):
        super(SelfAttentionPooling, self).__init__()
        # å¯å­¦ä¹ çš„çº¿æ€§å±‚ï¼Œç”¨äºè®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„æ³¨æ„åŠ›åˆ†æ•°
        self.attn_score = nn.Linear(embed_dim, 1)
    def forward(self, x):
        # x: [B, L, embed_dim]  ï¼ˆè¾“å…¥ç‰¹å¾éœ€å…ˆè°ƒæ¢ç»´åº¦åˆ° [æ—¶é—´æ­¥, ç‰¹å¾]ï¼‰
        scores = self.attn_score(x)                 # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°: [B, L, 1]
        weights = torch.softmax(scores, dim=1)      # å¯¹æ—¶é—´ç»´åº¦åšsoftmaxå½’ä¸€åŒ–
        context = (x * weights).sum(dim=1)          # åŠ æƒæ±‚å’Œå¾—åˆ°ä¸Šä¸‹æ–‡å‘é‡: [B, embed_dim]
        return context

# å•ä¸ªçº¿ç´¢åˆ†æ”¯æ¨¡å—ï¼šConv1D æå–ç‰¹å¾ + æ³¨æ„åŠ›æ± åŒ–å¾—åˆ°çº¿ç´¢è¡¨ç¤ºå‘é‡
class CueBranch(nn.Module):
    def __init__(self, input_channels=32, conv_channels=64, kernel_size=5, stride=1, num_layers=2, embed_dim=None, use_batchnorm=True):
        super(CueBranch, self).__init__()
        self.conv_net = ConvBranch(input_channels, conv_channels, kernel_size, stride, num_layers, use_batchnorm)
        # è®¾ç½®åµŒå…¥ç»´åº¦ï¼šè‹¥æœªæŒ‡å®šåˆ™ä¸å·ç§¯è¾“å‡ºé€šé“æ•°ç›¸åŒ
        self.embed_dim = conv_channels if embed_dim is None else embed_dim
        # è‹¥éœ€è¦å°†å·ç§¯è¾“å‡ºæ˜ å°„åˆ°ä¸åŒçš„embed_dimï¼Œå¯æ·»åŠ çº¿æ€§å±‚
        if embed_dim is not None and embed_dim != conv_channels:
            self.proj = nn.Linear(conv_channels, embed_dim)
        else:
            self.proj = None
        self.attn_pool = SelfAttentionPooling(self.embed_dim)
    def forward(self, x):
        # x: [B, input_channels, L]
        feat = self.conv_net(x)               # å·ç§¯æå–ç‰¹å¾: [B, conv_channels, L]
        feat = feat.transpose(1, 2)           # è°ƒæ•´ä¸º [B, L, conv_channels] ä»¥æ–¹ä¾¿æ³¨æ„åŠ›è®¡ç®—
        if self.proj is not None:
            feat = self.proj(feat)           # å¯é€‰ï¼šæ˜ å°„åˆ°æŒ‡å®šçš„åµŒå…¥ç»´åº¦ [B, L, embed_dim]
        cue_vector = self.attn_pool(feat)     # æ³¨æ„åŠ›æ± åŒ–å¾—åˆ°çº¿ç´¢ä¸Šä¸‹æ–‡å‘é‡: [B, embed_dim]
        return cue_vector

# ä¸»æ¨¡å‹ï¼šåŒ…å«ä¸‰ä¸ªçº¿ç´¢åˆ†æ”¯ã€è·¨çº¿ç´¢è‡ªæ³¨æ„åŠ›å±‚å’Œæœ€ç»ˆåˆ†ç±»å™¨
class SoundLocalizationModel(nn.Module):
    def __init__(self, num_classes=72, input_channels_per_cue=32, conv_channels=64, kernel_size=5, stride=1,
                 num_layers=2, embed_dim=64, num_heads=4, use_batchnorm=True):
        super(SoundLocalizationModel, self).__init__()
        # ä¸‰ä¸ªç‹¬ç«‹çš„çº¿ç´¢åˆ†æ”¯
        self.cue_branches = nn.ModuleList([
            CueBranch(input_channels_per_cue, conv_channels, kernel_size, stride, num_layers, embed_dim, use_batchnorm)
            for _ in range(3)
        ])
        # è·¨çº¿ç´¢å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚ï¼Œå°† embed_dim ç»´çš„3ä¸ªçº¿ç´¢å‘é‡ä½œä¸ºåºåˆ—
        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)
        self.norm = nn.LayerNorm(embed_dim)            # å±‚å½’ä¸€åŒ–ï¼Œè§„èŒƒæ³¨æ„åŠ›è¾“å‡º
        # å…¨è¿æ¥åˆ†ç±»å±‚ï¼Œå°†3*embed_dimæ˜ å°„ä¸ºnum_classesï¼ˆ72ä¸ªæ–¹ä½è§’ç±»åˆ«ï¼‰
        self.fc = nn.Linear(embed_dim * 3, num_classes)
    def forward(self, x):
        # x: [B, 3, 32, L]  å…¶ä¸­3è¡¨ç¤ºä¸‰ä¸ªçº¿ç´¢é€šé“ (ITD, ILD, IC)
        B, cue_count, C, L = x.shape
        assert cue_count == 3, "æ¨¡å‹æœŸæœ›è¾“å…¥åŒ…å«3ä¸ªçº¿ç´¢é€šé“"
        # åˆ†åˆ«é€šè¿‡æ¯ä¸ªçº¿ç´¢åˆ†æ”¯æå–è¡¨ç¤ºå‘é‡
        cue_vectors = []  # å°†æ”¶é›†æ¯ä¸ªåˆ†æ”¯è¾“å‡º [B, embed_dim]
        for i, branch in enumerate(self.cue_branches):
            cue_input = x[:, i]                   # å–å‡ºç¬¬ i ä¸ªçº¿ç´¢: [B, 32, L]
            vec = branch(cue_input)               # å¾—åˆ°è¯¥çº¿ç´¢çš„è¡¨ç¤ºå‘é‡: [B, embed_dim]
            cue_vectors.append(vec)
        # å°†ä¸‰ä¸ªçº¿ç´¢å‘é‡å †å æˆåºåˆ—ï¼Œå½¢çŠ¶ [B, 3, embed_dim]
        seq = torch.stack(cue_vectors, dim=1)
        # è‡ªæ³¨æ„åŠ›å±‚ï¼šè®©æ¯ä¸ªçº¿ç´¢å‘é‡ä¸å…¶ä»–çº¿ç´¢äº¤äº’å¾—åˆ°æ–°çš„è¡¨ç¤º
        attn_out, _ = self.cross_attn(seq, seq, seq)   # [B, 3, embed_dim]
        attn_out = self.norm(attn_out)                 # å±‚å½’ä¸€åŒ–è¾“å‡º
        # å°†3ä¸ªçº¿ç´¢å‘é‡å±•å¹³ä¸ºå•ä¸€å‘é‡ [B, 3*embed_dim]
        combined = attn_out.reshape(B, -1)
        # å…¨è¿æ¥åˆ†ç±»ï¼Œè¾“å‡º72ç»´ç±»åˆ«åˆ†æ•°
        logits = self.fc(combined)                     # [B, 72]
        # æ¨¡å‹è¾“å‡ºä¸ºæœªå½’ä¸€åŒ–çš„å¾—åˆ†ï¼Œå¯åœ¨éœ€è¦æ—¶ä½¿ç”¨ Softmax åšå½’ä¸€åŒ–:
        # probs = torch.softmax(logits, dim=1)
        return logits

random.seed(42)
random_ids = random.sample(range)
full_dataset = BinauralCueDataset(r"C:\Users\TIANY1\OneDrive - Trinity College Dublin\Documents\SoundSourceLocalization\features")
train_dataset, val_dataset = random_split(full_dataset, [0.8, 0.2], generator=torch.Generator().manual_seed(42))

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("The device type is: ", device)
model = model = SoundLocalizationModel(num_classes=72, input_channels_per_cue=32, conv_channels=64, kernel_size=5,
                               stride=1, num_layers=2, embed_dim=64, num_heads=4, use_batchnorm=True).to(device)
model_path = r"C:\Users\TIANY1\OneDrive - Trinity College Dublin\Documents\SoundSourceLocalization\checkpoints\model2\epoch_20.pth"
checkpoint = torch.load(model_path, map_location=device)
model.load_state_dict(checkpoint["model_state_dict"])
model.eval()


val_path = r"C:\Users\TIANY1\OneDrive - Trinity College Dublin\Documents\SoundSourceLocalization\features"
val_dataset = BinauralCueDataset(val_path, audio_ids=range(561, 701))
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)


all_preds, all_labels = [], []

with torch.no_grad():
    for X, y in val_loader:
        X, y = X.to(device), y.to(device)
        outputs = model(X)
        preds = outputs.argmax(dim=1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(y.cpu().numpy())

all_preds = np.array(all_preds)
all_labels = np.array(all_labels)

accuracy = np.mean(all_preds == all_labels)
print(f"Top-1 Accuracy: {accuracy * 100:.2f}%")

def mean_class_accuracy(y_true, y_pred, num_classes=72):
    class_accs = []
    for cls in range(num_classes):
        cls_mask = (y_true == cls)
        if cls_mask.sum() == 0: continue
        acc = (y_pred[cls_mask] == y_true[cls_mask]).sum() / cls_mask.sum()
        class_accs.append(acc)
    return np.mean(class_accs)

mean_acc = mean_class_accuracy(all_labels, all_preds)
print(f"Mean Accuracy per Class: {mean_acc * 100:.2f}%")

from sklearn.metrics import classification_report
print(classification_report(all_labels, all_preds, digits=3))

cm = confusion_matrix(all_labels, all_preds)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, cmap="Blues")
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

